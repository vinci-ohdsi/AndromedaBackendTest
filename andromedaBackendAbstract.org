#+TITLE: SQLite vs. DuckDB Benchmarking for the Andromeda Back-end Database 
#+AUTHOR: Bill O'Brien
:options-drawer:
#+OPTIONS: ^:nil H:5 num:nil
#+PROPERTY: header-args:R 
#+startup: indent visual
:END:

* Analysis
** Setup

Set org-mode global vars for use across R renv's within this notebook 

Number of /batchApply()/ repetitions 
#+NAME: nreps
| 5      | 

Simulate data and define list of test functions

#+begin_src R :session s1 :dir c:/andromedaDatabaseAbstract 

  library(dplyr)

  nPersons <- 500000
  nCovariates <- 150 

  fns <- list(mean = \(x) mean(x$covariateValue),
              filter = \(x) sum(x$covariateValue == 1L),
              groupSumm = \(x) dplyr::group_by(x, cohortId) %>%
                               dplyr::summarize(meanValue = mean(x$covariateValue)))

  df <- expand.grid(1:nPersons, 1:nCovariates) %>%
    setNames(c('personId', 'covariateId')) %>%
    mutate(covariateValue = sample(0:1L, size = nrow(.), prob = c(.9, .1), replace = TRUE),
           cohortId = sample(0:1L, size = nrow(.), prob = c(.5, .5), replace = TRUE))

  saveRDS(df, './df.RDS') 
  saveRDS(fns, './fns.RDS')

#+end_src

#+RESULTS:

** SQLite back-end

This code block uses the SQLite backend from the Andromeda main branch

#+begin_src R :session andromeda-sqlite :dir ./andromedaSqlite :exports code :var nreps=nreps 

  
  library(Andromeda)
  df <- readRDS('../df.RDS') 
  fns <- readRDS('../fns.RDS') 

  andromeda <- andromeda()
  andromeda$data <- df

  runTest <- function() {
    results <- sapply(fns,
                      \(f) {
                        start <- Sys.time()
                        batchApply(andromeda$data, fun = f,
                                   batchSize = 100000)
                        difftime(Sys.time(), start, units = "secs")
                      } 
                      )

    data.frame(db = 'SQLite', fn = names(fns), runTimeSeconds = results, row.names = NULL) 
  }

  saveRDS(Reduce(bind_rows, lapply(1:nreps[1,1], \(x) runTest())),
          '../sqlite-times.RDS')

#+end_src

#+RESULTS:


#+begin_src R :session andromeda-sqlite :dir ./andromedaSqlite :results output :exports both 
  
  andromeda 

#+end_src

#+RESULTS:
: [90m# Andromeda object[39m
: [90m# Physical location:  C:\Users\VHASLC~1\AppData\Local\Temp\1\RtmpuE4Qqg\file43284d105eac.sqlite[39m
: 
: Tables:
: $data (personId, covariateId, covariateValue, cohortId)


** DuckDB back-end

This uses the DuckDB backend from the Andromeda duckdb2 branch 

#+name: duckdb-times 
#+begin_src R :session andromeda-duckdb :dir ./andromedaDuckDb  :exports code  :var nreps=nreps

  library(Andromeda)
  df <- readRDS('../df.RDS') 
  fns <- readRDS('../fns.RDS') 

  andromeda <- andromeda()
  andromeda$data <- df

  runTest <- function() {
    results <- sapply(fns,
                      \(f) {
                        start <- Sys.time()
                        batchApply(andromeda$data, fun = f,
                                   batchSize = 100000)
                        difftime(Sys.time(), start, units = "secs")
                      } 
                      )

    data.frame(db = 'DuckDB', fn = names(fns), runTimeSeconds = results, row.names = NULL) 
  }

  saveRDS(Reduce(bind_rows, lapply(1:nreps[1,1], \(x) runTest())),
          '../duckdb-times.RDS')

#+end_src

#+RESULTS: duckdb-times


#+begin_src R :session andromeda-duckdb :dir ./andromedaDuckDB :results output :exports both 
  
  andromeda 

#+end_src

#+RESULTS:
: [90m# Andromeda object[39m
: [90m# Physical location:  C:\Users\VHASLCObrieW1\AppData\Local\Temp\1\Rtmp2LqKQc\file4aec18ed649b.duckdb[39m
: 
: Tables:
: $data (personId, covariateId, covariateValue, cohortId)


** batchApply results 

#+begin_src R :session s1 :results graphics :file ./boxplot.jpg 

  library(ggplot2)
  library(tidyr)

  rbind(readRDS('./sqlite-times.RDS'),
        readRDS('./duckdb-times.RDS')) %>%
    ggplot() +
    geom_boxplot(aes(db, y = runTimeSeconds)) +
    facet_wrap(~fn) + 
    labs(title = 'Execution time for benchmark',
         y = 'Seconds')  

#+end_src

#+RESULTS:

[[./boxplot.jpg]] 


** saveAndromeda results
*** SQLite 
#+begin_src R :session andromeda-sqlite :exports both 
  
  start <- Sys.time()
  saveAndromeda(andromeda, './andromeda.zip')
  sprintf("SQLite save time was %.01f seconds",
          difftime(Sys.time(), start, units = "secs")) 

#+end_src

#+RESULTS:
: SQLite save time was 23.8 seconds

*** DuckDB
#+begin_src R :session andromeda-duckdb :exports both 

  start <- Sys.time()
  saveAndromeda(andromeda, './andromeda.zip')
  sprintf("DuckDB save time was %.01f seconds",
          difftime(Sys.time(), start, units = "secs"))

#+end_src

#+RESULTS:
: DuckDB save time was 1.1 seconds

** loadAndromeda results 
*** SQLite

#+begin_src R :session andromeda-sqlite :exports both 

  start <- Sys.time()
  loadAndromeda('./andromeda.zip')
  sprintf("SQLite load time was %.01f seconds",
          difftime(Sys.time(), start, units = "secs")) 

#+end_src

#+RESULTS:
: SQLite load time was 8.3 seconds

*** DuckDB


#+begin_src R :session andromeda-duckdb :exports both 

  start <- Sys.time()
  loadAndromeda('./andromeda.zip')
  sprintf("DuckDB load time was %.01f seconds",
          difftime(Sys.time(), start, units = "secs"))

#+end_src

#+RESULTS:
: DuckDB load time was 0.4 seconds

** DB file size on disk
*** SQLite

#+begin_src R :session andromeda-sqlite :exports both

  file.remove(list.files(pattern = "*.sqlite"))

  unzip('./andromeda.zip') 

  sprintf("SQLite database file is %.01f MB",
          file.size(list.files(pattern = "\\.sqlite$"))/1E6)

#+end_src

#+RESULTS:
: SQLite database file is 1212.1 MB

*** DuckDB

#+begin_src R :session andromeda-duckdb :exports both 
  file.remove(list.files(pattern = "*.duckdb"))

  unzip('./andromeda.zip') 

  sprintf("Duckdb database file is %.01f MB",
          file.size(list.files(pattern = "\\.duckdb$"))/1E6)
  
#+end_src

#+RESULTS:
: Duckdb database file is 25.2 MB




* Abstract for OHDSI                                       :ignore:no_export:

** Background

The /Andromeda/ R package is a crucial component of OHDSI's Hades tool stack, allowing analysis of larger-than-memory data objects. The current implementation of /Andromeda/ uses the venerable /SQLite/ as its database back-end. /SQLite/ is a widely deployed database engine that is highly scalable, requires no server process or configuration, and has convenient R interop through /RSQLite/. While SQLite's row oriention is suited for transactional workloads, other databases may offer better analytic performance through column orientation. The purpose of this abstract is to compare the performance of Andromeda's current SQLite back-end with an alternative implementation that uses the DuckDB database. 

** Methods

The maintainer of Andromeda (A.B.) modified the package to use /duckdb-r/ in place of /RSQLite/ (both available on CRAN). We created 2 isolated R environments using /renv/. Lock files are available at github.com/vinci-ohdsi/AndromedaBackendTest. The first used the Andromeda main branch of github.com/OHDSI/Andromeda, and the second used the duckdb2 branch.

*** Artificial Data

We simulated a large database table containing columns for person, covariate ID, and covariate value. All data were randomly generated and without PHI. Three R functions were specified: filtering, calculating mean, and grouping/summarizing. These functions were applied to the database table using Andromeda's /batchApply()/, which uses /dbplyr/ to apply the supplied functions in-database, then results were retrieved as R objects in batches of 100,000. The exercise was repeated 5 times to obtain a distribution of run times. We then computed run times for /saveAndromeda()/, /loadAndromeda()/, and noted the database file sizes on disk. 

*** Real World Data 

Similar to the first exercise, we created 2 R environments within a Dept. of Veterans Affairs secure server. We then ran a model study (Ryan 2024) comparing the incidence of GI bleed with inpatient admission in new users of celecoxib vs. diclofenac. We then ran an analysis specified in /Strategus/ comparing incidence of X in new users of GLP1 ???...

** Results
#+name: time-results 
#+begin_src R :session s1 :results output raw :exports results

  s <- readRDS("./sqlite-times.RDS")

  sr <- sprintf("SQLite median (SD) [Figure 1] run times for calculating mean, filtering, and grouping/summarizing were %.01f (%.01f), %.01f (%.01f), %.01f (%.01f). ",
                median(s$runTimeSeconds[s$fn == 'mean']),
                sd(s$runTimeSeconds[s$fn == 'mean']),
                median(s$runTimeSeconds[s$fn == 'filter']),
                sd(s$runTimeSeconds[s$fn == 'filter']),
                median(s$runTimeSeconds[s$fn == 'groupSumm']),
                sd(s$runTimeSeconds[s$fn == 'groupSumm']))

  d <- readRDS("./duckdb-times.RDS")

  dr <- sprintf("Respective DuckDB times were %.01f (%.01f), %.01f (%.01f), %.01f (%.01f).",
                median(d$runTimeSeconds[d$fn == 'mean']),
                sd(d$runTimeSeconds[d$fn == 'mean']),
                median(d$runTimeSeconds[d$fn == 'filter']),
                sd(d$runTimeSeconds[d$fn == 'filter']),
                median(d$runTimeSeconds[d$fn == 'groupSumm']),
                sd(d$runTimeSeconds[d$fn == 'groupSumm']))

  cat(paste(sr, dr), "\n")

#+end_src

#+RESULTS: time-results
SQLite median (SD) [Figure 1] run times for calculating mean, filtering, and grouping/summarizing were 244.5 (7.6), 244.8 (8.8), 247.4 (7.3).  Respective DuckDB times were 19.1 (1.0), 19.7 (0.3), 23.1 (0.5)

[[./boxplot.jpg]]

#+NAME: results-tbl        
| Test ID | Execution (hrs) | Study | Database | tidyCovariates | createPs | T-cohort n | C-cohort n |
|---------+-----------------+-------+----------+----------------+----------+------------+------------|
|       1 |                 | Coxib | SQLite   |                |          |            |            |
|       2 |                 | Coxib | DuckDB   |                |          |            |            |
|       3 |                 | GLP1  | SQLite   |                |          |            |            |
|       4 |                 | GLP1  | DuckDB   |                |          |            |            |




** References
Schuemie M, Suchard M, Ryan P (2024). CohortMethod: New-User Cohort Method with Large Scale Propensity and Outcome
Models. R package version 5.4.0,
https://github.com/OHDSI/CohortMethod


* Project management                                              :no_export:
:LOGBOOK:
CLOCK: [2025-01-08 Wed 09:00]
CLOCK: [2025-01-07 Tue 09:47]--[2025-01-07 Tue 17:21] =>  7:34
CLOCK: [2025-01-06 Mon 12:32]--[2025-01-06 Mon 17:00]  =>  4:28
CLOCK: [2025-01-06 Mon 08:58]--[2025-01-06 Mon 12:23] =>  3:25
CLOCK: [2024-12-26 Thu 16:31]--[2024-12-26 Thu 17:00] =>  0:29
:END:

